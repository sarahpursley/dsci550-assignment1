{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = cyan> NEED TO FIGURE OUT WAY TO GRAB ALL FILES USING NOTEBOOK</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 550 Assignment 1\n",
    "### Sunday 03/14/21\n",
    "\n",
    "#### Group Members\n",
    "- Olivia Fryt <fryt@usc.edu> \n",
    "- Katie Chak <chakw@usc.edu>\n",
    "- Madeleine Thompson <mjt43250@usc.edu> \n",
    "- Sarah Pursley <spursley@usc.edu> \n",
    "- Amber Yu <qiuyunyu@usc.edu> \n",
    "- Claudia Winarko <cpwinark@usc.edu> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download/Install Apache Tika\n",
    "1. Start GeoTopicParser-enabled Tika Server:\n",
    "2.  Run a GeoTopicParser-enabled Apache Tika Server on localhost:9998. \n",
    "3. Install python tika package:\n",
    "> `pip install tika`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<font color=red>\n",
    "Containerized in my repo: https://github.com/frytoli/geotopic-parser-enabled-tika-docker\n",
    "docker run -d -p 127.0.0.1:9998:9998 fryto/gtp-tika:latest</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download/Install D3.js\n",
    "- Package: http://d3js.org/\n",
    "- Visual gallery wiki: https://github.com/mbostock/d3/wiki/Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Download Nigerian Price Fraud Email DatasetÂ¶\n",
    "Source: https://www.kaggle.com/rtatman/fraudulent-email-corpus <br>\n",
    "**Local Paths: ./data/fraudulent_emails.txt, ./data/fraudulent_emails_copy.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "infile = 'fraudulent_emails_copy.txt'\n",
    "path_to_infile = os.path.join(os.getcwd(), 'data', infile)\n",
    "\n",
    "outfile = 'fraudulent_emails_olivia.json'\n",
    "path_to_outfile = os.path.join(os.getcwd(), 'data', outfile)\n",
    "\n",
    "# Prepare method to save a checkpoint of the data to the \"outfile\"\n",
    "import json\n",
    "\n",
    "def save_to_outfile(data):\n",
    "    with open(path_to_outfile, 'w') as outjson:\n",
    "        json.dump(data, outjson, indent=2)\n",
    "    print(f'Data saved to {path_to_outfile}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. \"Convert\" messages to json with Tika\n",
    "**NOTE:**\n",
    "Use of the Tika CLI yields different (and better) results than directly using the API to parse files. I can't figure out why this is. The Tika python CLI \"parse\" command uses the parseAndSave() method (https://github.com/chrismattmann/tika-python/blob/d692c0ffa6b85d099019de9b94888fb4c2a48040/tika/tika.py#L261) that in turn sends data to the parse1() method (https://github.com/chrismattmann/tika-python/blob/d692c0ffa6b85d099019de9b94888fb4c2a48040/tika/tika.py#L310) which interfaces with Tika to complete parsing. Interestingly enough, the from_file() parsing method in the Tika python library (https://github.com/chrismattmann/tika-python/blob/d692c0ffa6b85d099019de9b94888fb4c2a48040/tika/parser.py#L23) also passes data through the parse1() method to Tika. The only main difference between parseAndSave() and from_file() is the way the output data is handled and returned: the parseAndSave() method simply writes the item at index 1 in the returned json on a new line while the from_file() method sends the data returned from Tika to the _parse() method (https://github.com/chrismattmann/tika-python/blob/d692c0ffa6b85d099019de9b94888fb4c2a48040/tika/parser.py#L68) for additional parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tika    # import tika to parse files\n",
    "from tika import tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & parse json file using tika parser\n",
    "parsed = json.loads(tika.parse1('all', path_to_infile)[1])\n",
    "\n",
    "# Ignore the first returned item, as it is associated with the infile\n",
    "messages = parsed[1:]\n",
    "\n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Ensure each message is a separate json entity\n",
    "** Addition of new key embedded_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and attribute message attachments (these are parsed as separate json blobs)\n",
    "#   * Attachment (embedded resource) metadata is stored in the json object(s) directly before the main message object in the list returned from Tika\n",
    "resource_indicies = []\n",
    "resources = []\n",
    "count = 0\n",
    "\n",
    "for message in messages:\n",
    "    # If at least one resource needs to be added...\n",
    "    if len(resources) > 0:\n",
    "        \n",
    "        # If there seems to be another resource attached to the same message, append it to resources, save item index, and continue\n",
    "        if message['X-TIKA:embedded_depth'] != '1':\n",
    "            resource_indicies.append(count)\n",
    "            resources.append(message)\n",
    "            \n",
    "        # Else if the current message is the main message (aka it is not an embedded message)...\n",
    "        elif message['X-TIKA:embedded_depth'] == '1':\n",
    "            # Iterate over the resources...\n",
    "            for resource in resources:\n",
    "                # Assume the message (as long as it's immediately after the attachment(s)) is associated with the attachment(s)\n",
    "                if 'embedded_resources' not in message:\n",
    "                    message['embedded_resources'] = [resource]\n",
    "                else:\n",
    "                    message['embedded_resources'].append(resource)\n",
    "            # Reset\n",
    "            resources = []\n",
    "        else:\n",
    "            print('[!] Houston we have a problem')\n",
    "            # Reset\n",
    "            resources = []\n",
    "            \n",
    "    # otherwise, check if the current message is an embedded resource\n",
    "    elif message['X-TIKA:embedded_depth'] != '1':\n",
    "        resource_indicies.append(count)\n",
    "        resources = [message]\n",
    "    \n",
    "    count += 1    # increment the counter\n",
    "\n",
    "# Remove messages that are embedded resources -- best practice is not to manipulate a list while iterating over it\n",
    "delta = 0\n",
    "for index in resource_indicies:\n",
    "    del messages[index-delta]\n",
    "    delta +=1\n",
    "\n",
    "print(f'{len(resource_indicies)} embedded resources attributed to {len(messages)} total messages')\n",
    "\n",
    "# save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**If we need to see every fraud email content**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in messages:\n",
    "    if \"friend\" in message[\"X-TIKA:content\"]:\n",
    "        print(message[\"X-TIKA:content\"].lstrip().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Enrichment with Attack Type and Attacker Stylometrics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Analysis: Message Contents Language Identification\n",
    "- Before analysis, use Tika to identify the language of each message's body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Standard Python Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os and sys already called\n",
    "#!{sys.executable} -m pip install ipinfo\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Main Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for communicating with Tika Server\n",
    "def languagedetect_string(string):\n",
    "    r = requests.put('http://localhost:9998/language/string', data=string)\n",
    "    if r.status_code == 200:\n",
    "        return r.text\n",
    "\n",
    "\n",
    "whitespace_ptrn = re.compile(r'\\s')\n",
    "langs = dict()\n",
    "\n",
    "# Iterate over messages\n",
    "for message in messages:\n",
    "    # Normalize all whitespace in message body\n",
    "    norm_content = whitespace_ptrn.sub(' ', message['X-TIKA:content']).lower().encode('utf-8')\n",
    "    # Evaluate language with Tika\n",
    "    lang = languagedetect_string(norm_content)\n",
    "    if lang in langs:\n",
    "        langs[lang] += 1\n",
    "    else:\n",
    "        langs[lang] = 1\n",
    "    # Save language\n",
    "    message['TIKA-language'] = lang\n",
    "\n",
    "# Output the languages and counts\n",
    "for key, value in langs.items():\n",
    "    print(f'{key} : {value}')\n",
    "\n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a. Attack Type Classification\n",
    "- Begin by generating a word cloud to gain insight into the content of the messages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Main Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join and normalize all message contents and message subjects as strings\n",
    "whitespace_ptrn = re.compile(r'\\s')\n",
    "cloud_corpus = ' '.join(whitespace_ptrn.sub(' ', message['X-TIKA:content']).lower().strip() for message in messages)\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width = 3000,\n",
    "    height = 2000,\n",
    "    background_color = 'white',\n",
    "    stopwords = set(STOPWORDS),\n",
    "    random_state = 1,\n",
    "    collocations = False\n",
    ").generate(str(cloud_corpus))\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(1, figsize=(40, 30))\n",
    "plt.axis('off')\n",
    "fig.suptitle('Fig 1. Message Content', fontsize=40)\n",
    "fig.subplots_adjust(top=1.06)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorize the messages into the following social engineering types by \n",
    "1.  Scanning a compiled set of keywords \n",
    "2.  Recognizing suspicious attachments/resources (in other words, messages with an attachment are considered to be of SE type \"malware\"):\n",
    "\n",
    "**SE Types:** Reconnaissance, Social Engineering, Malware, Phishing\n",
    "\n",
    "**NOTE:** Intelligent categorization into these four categories is not possible without training data (must be supervised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_keywords = {\n",
    "    'recon': ['reply back', 'click here', 'confidential', 'contact', 'response', 'private', 'personal', 'information'],\n",
    "    'social_engineering': ['your friend', 'comrade', 'urgent', 'need help', 'threat', 'act now', '$', 'money', 'transaction', 'give', 'assist', 'business', 'fund', 'help', 'unclaimed', 'please', 'died'],\n",
    "    'malware': ['click here', 'http'],\n",
    "    'phishing': ['social security', 'ssn', 'date of birth', 'birthday', 'dob', 'account number', 'bank', 'beneficiary', 'password', 'phone number', 'fax number', 'company address']\n",
    "}\n",
    "fr_keywords = {} # TBD!\n",
    "es_keywords = {} # TBD!\n",
    "\n",
    "# Keep count of keywords to get more insight/clean keywords\n",
    "kw_count = {kw:0 for kw in keywords}\n",
    "\n",
    "# Tag messages\n",
    "for message in messages:\n",
    "    message['se_tags'] = []\n",
    "    # If message is in French, use French keywords\n",
    "    if message['TIKA-language'] == 'fr':\n",
    "        keywords = fr_keywords\n",
    "    # Else if message is in Spanish, use Spanish keywords\n",
    "    elif message['TIKA-language'] == 'es':\n",
    "        keywords = es_keywords\n",
    "    # Otherwise, use English keywords (manual review shows that all other languages also have an English part AND I don't know anything about the other languages)\n",
    "    else:\n",
    "        keywords = en_keywords\n",
    "    # Find keywords in message body\n",
    "    for kw_type in keywords:\n",
    "        for kw in keywords[kw_type]:\n",
    "            if kw in message['X-TIKA:content']:\n",
    "                if kw_type not in message['se_tags']:\n",
    "                    message['se_tags'].append(kw_type)\n",
    "                    kw_count[kw_type] += 1\n",
    "    # If the message contains at least one attachment, classify it as \"malware\"\n",
    "    if 'embedded_resources' in message:\n",
    "        if 'malware' not in message['se_tags']:\n",
    "            message['se_tags'].append('malware')\n",
    "            kw_count['malware'] += 1\n",
    "        \n",
    "\n",
    "print('Count of messages by tag:')\n",
    "for kw in sorted(kw_count, key=kw_count.get, reverse=True):\n",
    "    print(f'  {kw}: {kw_count[kw]}')\n",
    "\n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.b. Attacker Stylometric Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Find attacker title\n",
    "**NOTE:** Named entity recognition with NLTK does not notice titles without personalization. Therefore, fuzzy string matching was utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Main Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of sender titles\n",
    "titles = ['mr', 'mrs', 'miss', 'ms', 'madame', 'honourable', 'lady', 'senator', 'governor', 'hon', 'col', 'colonel',\n",
    "          'president', 'partner', 'senior', 'ceo', 'executive', 'officer', 'chief', 'maitre', 'dr', 'doctor', 'barr',\n",
    "          'barrister', 'esq', 'esquire', 'manager', 'executive', 'sister', 'brother', 'captain', 'capt', 'head', 'lead',\n",
    "          'director', 'prince', 'princess', 'king', 'queen', 'chairman', 'judge', 'engr', 'engineer', 'minister']\n",
    "abrv_titles = {\n",
    "    'dr': 'doctor',\n",
    "    'barr': 'barrister',\n",
    "    'esq': 'esquire',\n",
    "    'capt': 'captain',\n",
    "    'engr': 'engineer',\n",
    "    'ceo': 'chief executive officer',\n",
    "    'hon': 'honourable',\n",
    "    'col': 'colonel'\n",
    "}\n",
    "# Note that CEO is a bit strange, as the title can be set from three different titles\n",
    "#  ceo = all(['chief', 'executive', 'officer'])\n",
    "\n",
    "# Iterate over messages\n",
    "for message in messages:\n",
    "    # Create \"author_titles\" key-value pair\n",
    "    message['author_titles'] = []\n",
    "    temp_titles = set()\n",
    "    # Fuzzy match author name, if applicable\n",
    "    if 'Author' in message:\n",
    "        # If applicable, pick last author in author string to fuzzy match on (this item seems to have been parsed more cleanly by Tika)\n",
    "        if len(message['Author']) > 1:\n",
    "            author = message['Author'][-1].lower()\n",
    "        else:\n",
    "            author = message['Author'][0].lower()\n",
    "        # Split author string by space, period, comma, and dash, and normalize (lowercase)\n",
    "        split_author = re.split('[ .,-]', author.lower())\n",
    "        # Iterate over titles\n",
    "        for title in titles:\n",
    "            # Iterate over split parts of author\n",
    "            for part in split_author:\n",
    "                # Fuzzy match the parse parts of the author string with each title\n",
    "                ratio = fuzz.ratio(title, part)\n",
    "                if ratio > 86: # 86 seems to be the threshold at which \"mrs\" != \"mr\" and \"prince\" != \"princess\"\n",
    "                    # Add title to set\n",
    "                    temp_titles.add(title)\n",
    "    # If not title was found in the author name or an author name does not exist, separate message content at last comma, split, and fuzzy match\n",
    "    if 'Author' not in message or len(temp_titles) == 0:\n",
    "        indexof_last_comma = message['X-TIKA:content'].rfind(',')\n",
    "        endof_body = message['X-TIKA:content'][indexof_last_comma+1:]\n",
    "        # Split string by space, period, comma, and dash, and normalize (lowercase)\n",
    "        split_endof_body = re.split('[\\s.,\\-()]', endof_body.lower())\n",
    "        # Iterate over titles\n",
    "        for title in titles:\n",
    "            # Iterate over split parts of body\n",
    "            for part in split_endof_body:\n",
    "                # Fuzzy match the parse parts of the author string with each title\n",
    "                ratio = fuzz.ratio(title, part)\n",
    "                if ratio > 86: # 86 seems to be the threshold at which \"mrs\" != \"mr\" and \"prince\" != \"princess\"\n",
    "                    # Add title to set\n",
    "                    temp_titles.add(title)\n",
    "    # Make titles consistent (handle abbreviations)\n",
    "    # First a foremost, lets see if author is a ceo ** Note the comment above\n",
    "    ceo_check = all(item in list(temp_titles) for item in ['chief', 'executive', 'officer'])\n",
    "    if ceo_check:\n",
    "        temp_titles.add('ceo')\n",
    "        temp_titles.remove('chief')\n",
    "        temp_titles.remove('executive')\n",
    "        temp_titles.remove('officer')\n",
    "    # Iterate over titles and add to message blob\n",
    "    for title in temp_titles:\n",
    "        try:\n",
    "            message['author_titles'].append(abrv_titles[title])\n",
    "        except:\n",
    "            message['author_titles'].append(title)\n",
    "\n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii.  Attribute a level of urgency (\"urgency score\") to each message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Main Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Synonyms/antonyms of \"urgent\" from thesaurus.com: https://www.thesaurus.com/browse/urgent\n",
    "#  Some words removed to avoid confusion, mainly in regards to sender titles i.e. \"chief\" and \"principal\"\n",
    "#  Some words added after manual review of messages\n",
    "\n",
    "urgency_ratings = {\n",
    "    2: ['urgency', 'urgent', 'compelling', 'compel', 'critical', 'crucial', 'demanding', 'essential', 'immediate',\n",
    "        'imperative', 'important', 'indispensable', 'necessary', 'persuasive', 'pressing', 'serious', 'vital', 'weighty',\n",
    "        'as soon as possible', 'may not exceed', 'now'],\n",
    "    1: ['burning', 'called-for', 'clamant', 'clamorous', 'crying', 'demanded', 'driving', 'exigent', 'foremost', 'heavy',\n",
    "        'hurry-up', 'impelling', 'importunate', 'insistent', 'instant', 'leading', 'life and death', 'momentous', 'paramount',\n",
    "        'primary', 'required', 'salient', 'top-priority', 'touch and go', 'touchy', 'wanted', 'quick', 'earliest response',\n",
    "        'at once', 'right now'],\n",
    "    -1: ['easy', 'facile', 'inessential', 'insignificant', 'light', 'minor', 'needless', 'nonessential', 'optional',\n",
    "         'secondary', 'small', 'trivial', 'uncritical', 'unimportant', 'unnecessary', 'unsubstantial', 'voluntary',\n",
    "         'eventually', 'later']\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Iterate over messages\n",
    "for message in messages:\n",
    "    # Iterate over words in urgency ratings, see if they exist in the message's body, and compile a score accordingly\n",
    "    message['urgency_score'] = 0\n",
    "    for score in urgency_ratings:\n",
    "        for word in urgency_ratings[score]:\n",
    "            # If the word is found in the message's body content, add the associated score (note that antonyms take away from the overall urgency score)\n",
    "            if word in message['X-TIKA:content'].lower():\n",
    "                message['urgency_score'] += score\n",
    "    scores.append(message['urgency_score'])\n",
    "\n",
    "# Output max, avg, min of scores\n",
    "print(f'Max \"urgency score\": {max(scores)}')\n",
    "print(f'Avg \"urgency score\": {sum(scores)/len(scores)}')\n",
    "print(f'Min \"urgency score\": {min(scores)}')\n",
    "\n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Find the date/time of the email attack\n",
    "Convert message creation timestamps to timezone-aware (if possible) ISO 8601 format strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "from dateutil import parser\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Main Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile regex patterns\n",
    "whitespace_ptrn = re.compile(r'\\s')\n",
    "mboxfrom_date_ptrn = re.compile(r'(Mon|Tue|Wed|Thu|Fri|Sat|Sun) {1,2}(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)( [1-9][0-9]|  [1-9]) [0-9]{2}:[0-9]{2}:[0-9]{2} [1-2][0-9]{3}')\n",
    "date_ptrn = re.compile(r'(Mon|Tue|Wed|Thu|Fri|Sat|Sun), [1-9][0-9]{0,1} (Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) [1-2][0-9]{3} [0-9]{2}:[0-9]{2}:[0-9]{2} -[0-9]{4} \\([A-Z]{3}\\)')\n",
    "\n",
    "nodate = []\n",
    "\n",
    "# Iterate over messages\n",
    "for message in messages:\n",
    "    # Keep track of datetime object itself\n",
    "    dt = None\n",
    "    # If \"Creation-Date\" in message record, convert value directly to isoformat\n",
    "    if 'Creation-Date' in message:\n",
    "        # Convert \"Creation-Date\" to ISO 8601\n",
    "        dt = parser.isoparse(message['Creation-Date'])\n",
    "    # Otherwise, if \"MboxParser-from\" in message record, convert this value to isoformat AND use previously discovered geolocation for timezone\n",
    "    elif 'MboxParser-from' in message:\n",
    "        # Prioritize IP address timezone, then Tika GeoTopicParser locations\n",
    "        if 'IPInfo-data' in message:\n",
    "            # Normalize value to parse\n",
    "            mboxparser_from = whitespace_ptrn.sub(' ', message['MboxParser-from'])\n",
    "            # Parse date from value\n",
    "            dtstr = mboxfrom_date_ptrn.search(mboxparser_from)\n",
    "            if dtstr:\n",
    "                dt = parser.parse(dtstr.group(0))\n",
    "                # Update with timezone aware value\n",
    "                tz = timezone(message['IPInfo-data']['timezone'])\n",
    "                dt = tz.localize(dt)\n",
    "        # Else parse date from 'MboxParser-return-path' or from 'MboxParser-x-sieve'/'MboxParser-from' if possible\n",
    "        elif 'MboxParser-return-path' or 'MboxParser-x-sieve' in message:\n",
    "            # 'MboxParser-return-path' logic\n",
    "            if 'MboxParser-return-path' in message:\n",
    "                # Handle potential types of 'MboxParser-return-path' value\n",
    "                if type(message['MboxParser-return-path']) == str:\n",
    "                    # Look for date pattern in string\n",
    "                    dtstr = date_ptrn.search(message['MboxParser-return-path'])\n",
    "                    # If a match was found, parse it to datetime\n",
    "                    if dtstr:\n",
    "                        # Parse out date\n",
    "                        dt = parser.parse(dtstr.group(0))\n",
    "                        # Acknowledge that timestamp was found\n",
    "                        timestamp_found = True\n",
    "                elif type(message['MboxParser-return-path']) == list:\n",
    "                    # Iterate over items\n",
    "                    for path in message['MboxParser-return-path']:\n",
    "                        # Look for date pattern in string\n",
    "                        dtstr = date_ptrn.search(path)\n",
    "                        # If a match was found, parse it to datetime\n",
    "                        if dtstr:\n",
    "                            # Parse out date\n",
    "                            dt = parser.parse(dtstr.group(0))\n",
    "                            # Break out of loop\n",
    "                            break\n",
    "                else:\n",
    "                    print('[!] Houston we have a problem')\n",
    "            # 'MboxParser-x-sieve'/'MboxParser-from' logic\n",
    "            if not dt and 'MboxParser-x-sieve' in message:\n",
    "                # Look for date pattern in string\n",
    "                dtstr = date_ptrn.search(message['MboxParser-x-sieve'])\n",
    "                #\n",
    "                # It seems that the value of 'MboxParser-from' is typically just a couple minutes different than the value from\n",
    "                # 'MboxParser-x-sieve'. We'll check that this is the case, and if it is we'll use the timezone from 'MboxParser-x-sieve'\n",
    "                # and the datetime value from 'MboxParser-from'\n",
    "                #\n",
    "                # If a match was found...\n",
    "                if dtstr:\n",
    "                    # Extract timezone from dtstr\n",
    "                    tzstr = dtstr.group(0).split(' ')[-2]\n",
    "                    # Normalize 'MboxParser-from' value to parse\n",
    "                    mboxparser_from = whitespace_ptrn.sub(' ', message['MboxParser-from'])\n",
    "                    # Parse date from value\n",
    "                    dtstr = mboxfrom_date_ptrn.search(mboxparser_from)\n",
    "                    # Combine datetime and timezone strings, and parse to datetime object\n",
    "                    dt = parser.parse(f'{dtstr.group(0)} {tzstr}')\n",
    "    \n",
    "    # If no timestamp found yet, attempt to parse timezone unaware timestamp\n",
    "    if not dt:\n",
    "        # Normalize value to parse\n",
    "        mboxparser_from = whitespace_ptrn.sub(' ', message['MboxParser-from'])\n",
    "        # Parse date from value\n",
    "        dtstr = mboxfrom_date_ptrn.search(mboxparser_from)\n",
    "        if dtstr:\n",
    "            dt = parser.parse(dtstr.group(0))\n",
    "\n",
    "    # Store timestamp in message as ISO 8601 string\n",
    "    message['timestamp'] = dt.isoformat()\n",
    "\n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Claudia's Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodate = []\n",
    "\n",
    "# Iterate over messages\n",
    "for message in messages:\n",
    "    # If \"Creation-Date\" in message record, convert value directly to isoformat\n",
    "    if 'Creation-Date' in message:\n",
    "        # Convert \"Creation-Date\" to ISO 8601 and store in *** new key \"timestamp\" ***\n",
    "        message['timestamp'] = parser.isoparse(message['Creation-Date']).isoformat()\n",
    "    # Otherwise, if \"MboxParser-from\" in message record, convert this value to isoformat AND use previously discovered geolocation for timezone\n",
    "    elif 'MboxParser-from' in message:\n",
    "        # Prioritize IP address timezone, then Tika GeoTopicParser locations\n",
    "        \n",
    "    else:\n",
    "        print('[!] Houston we have a problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Categorize attacker's offering\n",
    "Offerrings: percentage of money, lump-sum of money, religious benefit, investment management opportunity, product for sale, loan, bank account, social media invitation (invitation from facebox), wanted (sender allegedly being arrested for something)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkblue>**Start by using k-means clustering to find natural clusters based on language to gain insight into the individual meanings behind the messages:**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_clusters(corpus, k=2):\n",
    "    # Using TfidfVectorizer and English stopwords (because majority of messages are in English)\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # Create model\n",
    "    model = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Get centroids and features\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Iterate and output\n",
    "    for i in range(k):\n",
    "        print(f'Cluster {i}:')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(f'  {terms[ind]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, join and normalize all message contents\n",
    "whitespace_ptrn = re.compile(r'\\s')\n",
    "corpus = [whitespace_ptrn.sub(' ', message['X-TIKA:content']).lower().strip() for message in messages]\n",
    "\n",
    "# Get insight from k-means clusters (mess with the value of k)\n",
    "see_clusters(corpus, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = red>**Katie's Notebook**</font>\n",
    "- Topic 1: asks for money for family emergency {t1:[\"relative\",\"deceased\",\"money\",]}\n",
    "- Topic 2: asks for money for charity {t2:[\"charity\",\"friend\",\"bless\"]}\n",
    "- Topic 3: asks to transfer money to Africa for foreign assistance {t3:[\"Africa\",\"funds\",\"contract\",\"foreign\"]}\n",
    "- Topic 4: asks for money for business investment funds {t4:[\"business\",\"investment\",\"transaction\",\"document\"]}\n",
    "- Topic 5: job offer from Tasaki-Shinju {t5:[\"tasaki\",\"shinju\",\"company\",\"family\",\"job\"]}\n",
    "- Topic 6: offering sum of money from bank in Nigeria {t6:[\"foreign\",\"fund\",\"nigeria\",\"country\"]}\n",
    "- Topic 7: need help to get sums of money (will split sum) {t7:[\"split\", \"unclaimed\",\"transfer\",\"claim\",\"father\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offerings={\"t1\":[\"relative\",\"deceased\",\"money\",\"family\"],\n",
    "           \"t2\":[\"charity\",\"friend\",\"bless\",\"cheque\",\"organization\",\"ramitez\"],\n",
    "           \"t3\":[\"Africa\",\"funds\",\"contract\",\"foreign\"],\n",
    "           \"t4\":[\"business\",\"investment\",\"transaction\",\"document\"],\n",
    "           \"t5\":[\"tasaki\",\"shinju\",\"company\",\"family\",\"job\"],\n",
    "           \"t6\":[\"foreign\",\"fund\",\"nigeria\",\"country\"],\n",
    "           \"t7\":[\"split\", \"unclaimed\",\"transfer\",\"claim\",\"father\"]}\n",
    "\n",
    "def get_ans(topic_wrd_cnt,total_word,topic_name):\n",
    "    ratio=topic_wrd_cnt/total_word\n",
    "    return (ratio,topic_name)    # returns tuple of topic name and its strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in messages:\n",
    "    # initializing topic ratios\n",
    "    t1=0\n",
    "    t2=0\n",
    "    t3=0\n",
    "    t4=0\n",
    "    t5=0\n",
    "    t6=0\n",
    "    t7=0\n",
    "    words=0\n",
    "    for k,v in offerings.items():\n",
    "        #print(k,v)\n",
    "        email=message[\"X-TIKA:content\"].lstrip().strip()\n",
    "        token=prepare_text_for_lda(email)\n",
    "        #print(token)\n",
    "        for word in token:\n",
    "            if word in v and k==\"t1\":\n",
    "                t1+=1\n",
    "            if word in v and k==\"t2\":\n",
    "                t2+=1\n",
    "            if word in v and k==\"t3\":\n",
    "                t3+=1\n",
    "            if word in v and k==\"t4\":\n",
    "                t4+=1\n",
    "            if word in v and k==\"t5\":\n",
    "                t5+=1\n",
    "            if word in v and k==\"t6\":\n",
    "                t6+=1\n",
    "            if word in v and k==\"t7\":\n",
    "                t7+=1\n",
    "            words+=1\n",
    "    t1=get_ans(t1,words,\"t1\")\n",
    "    t2=get_ans(t2,words,\"t2\")\n",
    "    t3=get_ans(t3,words,\"t3\")\n",
    "    t4=get_ans(t4,words,\"t4\")\n",
    "    t5=get_ans(t5,words,\"t5\")\n",
    "    t6=get_ans(t6,words,\"t6\")\n",
    "    t7=get_ans(t7,words,\"t7\")\n",
    "    total_list=[t1,t2,t3,t4,t5,t6,t7]\n",
    "    #print(total_list)\n",
    "    #print(total_list)\n",
    "    #sort list in ascending order\n",
    "    total_list.sort()\n",
    "    \n",
    "    message[\"offering\"]=total_list[-1]\n",
    "    #print(message[\"offering\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Identify attacker's location\n",
    "#### **Get geolocation with geoNames.txt**  \n",
    "- Use GeoTopicParser in Tika: https://cwiki.apache.org/confluence/display/tika/GeoTopicParser   \n",
    "\n",
    "**NOTES:**\n",
    "Was unable to get GeoTopicParser to work with Python Tika, so I put together and am using a containerized GeoTopicParser-enabled Tika Server (my repo: https://github.com/frytoli/geotopic-parser-enabled-tika-docker). \n",
    "1. Run this in the background (see step 1 above), then run code below:\n",
    "2. Find geolocations mentioned in each message's content with Tika's GeoLocationParser:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Main Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define method for communicating with GeoTopicParser-enabled Tika Server\n",
    "def geotopicparse_file(filename, filedata):\n",
    "    headers = {\n",
    "        'Content-Disposition': f'attachment; filename={filename}'\n",
    "    }\n",
    "    r = requests.put('http://localhost:9998/rmeta', headers=headers, data=filedata)\n",
    "    if r.status_code == 200:\n",
    "        return r.json()\n",
    "\n",
    "    \n",
    "whitespace_ptrn = re.compile(r'\\s')\n",
    "\n",
    "# Iterate over messages\n",
    "for message in messages:\n",
    "    # Normalize all whitespace in message body\n",
    "    norm_content = whitespace_ptrn.sub(' ', message['X-TIKA:content']).encode('utf-8')\n",
    "    # Parse each message content for geotopics\n",
    "    parsed = geotopicparse_file('message.geot', norm_content)\n",
    "    # Save extracted geolocations to message json\n",
    "    message['TIKA-GeoLocationParser'] = parsed\n",
    "\n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = DarkBlue>**Find geolocation based off of sender IP addresses using IPInfo (https://github.com/ipinfo/python) (note that IP addresses change, so this data might not be entirely accurate):**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ipinfo object and handler\n",
    "ipinfo_token = 'eb65e70ef00bb6' # 50,000 free requests per month\n",
    "handler = ipinfo.getHandler(ipinfo_token)\n",
    "\n",
    "# Compile regex patterns\n",
    "whitespace_ptrn = re.compile(r'\\s')\n",
    "ip_ptrn = re.compile(r'(?<![0-9])[1-9][0-9]{0,2}\\.[1-9][0-9]{0,2}\\.[1-9][0-9]{0,2}\\.[1-9][0-9]{0,2}(?![0-9])')\n",
    "\n",
    "# IP Parsing method\n",
    "def parse_ip(string):\n",
    "    # Normalize value to parse\n",
    "    wstring = whitespace_ptrn.sub(' ', string)\n",
    "    # Parse date from value\n",
    "    ip = ip_ptrn.search(wstring)\n",
    "    return ip\n",
    "\n",
    "# Iterate over messages\n",
    "for message in messages:\n",
    "    ip = None\n",
    "    if 'MboxParser-x-sender-ip' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['MboxParser-x-sender-ip'])\n",
    "    if 'MboxParser-x-senderip' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['MboxParser-x-senderip'])\n",
    "    if not ip and 'MboxParser-x-origination-ip' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['MboxParser-x-origination-ip'])\n",
    "    if not ip and 'Message:Raw-Header:X-ORIGINATE-IP' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['Message:Raw-Header:X-ORIGINATE-IP'])\n",
    "    if not ip and 'MboxParser-xoriginalsenderip' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['MboxParser-xoriginalsenderip'])\n",
    "    if not ip and 'MboxParser-x-original-ip' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['MboxParser-x-original-ip'])\n",
    "    if not ip and 'MboxParser-x-aol-ip' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['MboxParser-x-aol-ip'])\n",
    "    if not ip and 'MboxParser-x-php-remote_addr' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['MboxParser-x-php-remote_addr'])\n",
    "    if not ip and 'Message:Raw-Header:Message-ID' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['Message:Raw-Header:Message-ID'])\n",
    "    if not ip and 'MboxParser-xparam2' in message:\n",
    "        # Preprocess string and parse ip address\n",
    "        ip = parse_ip(message['MboxParser-xparam2'])\n",
    "    \n",
    "    if ip:\n",
    "        # Get ip address details from ipinfo\n",
    "        details = handler.getDetails(ip.group(0).strip())\n",
    "        # Retrieve and save all detail information\n",
    "        message['IPInfo-data'] = details.all\n",
    "\n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. Attacker's alleged relationship to the recipient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Main Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship={\"foreign_partner\":[\"foreign\",\"partner\",\"father\",\"majesty\",\"assistance\",\"sister\"],\n",
    "             \"stranger_in_need\":[\"wife\",\"husband\",\"son\",\"father\",\"sister\"],\n",
    "              \"friend\":[\"friend\",\"dear\",\"help\",\"assistance\",\"money\"]}\n",
    "\n",
    "\n",
    "for message in messages:\n",
    "    # initializing topic ratios\n",
    "    t1=0\n",
    "    t2=0\n",
    "    t3=0\n",
    "\n",
    "    words=0\n",
    "    for k,v in relationship.items():\n",
    "        #print(k,v)\n",
    "        email=message[\"X-TIKA:content\"].lstrip().strip()\n",
    "        token=prepare_text_for_lda(email)\n",
    "        #print(token)\n",
    "        for word in token:\n",
    "            if word in v and k==\"foreign_partner\":\n",
    "                t1+=1\n",
    "            if word in v and k==\"stranger_in_need\":\n",
    "                t2+=1\n",
    "            if word in v and k==\"friend\":\n",
    "                t3+=1\n",
    "        \n",
    "            words+=1\n",
    "    t1=t1/words\n",
    "    t2=t2/words\n",
    "    t3=t3/words\n",
    "\n",
    "    total_list=[t1,t2,t3]\n",
    "    #print(total_list)\n",
    "    #print(total_list)\n",
    "    total_list.sort()    # Sort list in ascending order\n",
    "    if total_list[-1]==t1:\n",
    "        message[\"relationship\"]=\"foreign_partner\"\n",
    "    elif total_list[-1]==t2:\n",
    "        message[\"relationship\"]=\"stranger_in_need\"\n",
    "    else:\n",
    "        message[\"relationship\"]=\"friend\"\n",
    "    print(message[\"relationship\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vii. Attacker email sentiment\n",
    "https://github.com/USCDataScience/SentimentAnalysisParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pre-trained NLTK sentiment analyzer \n",
    "sia = SentimentIntensityAnalyzer()\n",
    "for message in messages: \n",
    "    message['Sentiment_Analysis'] = sia.polarity_scores(message['X-TIKA:content'])\n",
    "    \n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viii. Attacker language style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Ratio of Misspelled Words** (https://github.com/mammothb/symspellpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Claudia's Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize symspell object\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# Find and load the dictonary resources\n",
    "dictionary_path = pkg_resources.resource_filename('symspellpy', 'frequency_dictionary_en_82_765.txt')\n",
    "bigram_path = pkg_resources.resource_filename('symspellpy', 'frequency_bigramdictionary_en_243_342.txt')\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# Compile regex patterns to ignore in message body contents\n",
    "email_ptrn = re.compile(r'(?!_)[\\w]{1,11}@(?![_\\-.])[\\w]{1,11}(\\.[a-zA-Z]{1,10}){1,2}')\n",
    "phone_ptrn = ''\n",
    "addrs_ptrn = ''\n",
    "\n",
    "# Iterate over messages and find incorrect spellings\n",
    "for message in messages:\n",
    "    suggestions = sym_spell.lookup_compound(message['content-body'], max_edit_distance=2, transfer_casing=True)\n",
    "    print(message['content-body'])\n",
    "    print('\\n')\n",
    "    for suggestion in suggestions:\n",
    "        print(suggestion)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Main Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install symspellpy\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "#import re\n",
    "\n",
    "# Initialize symspell object\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# !!! Account for other languages? I.e. French and Spanish messages\n",
    "# Find and load the dictonary resources\n",
    "dictionary_path = pkg_resources.resource_filename('symspellpy', 'frequency_dictionary_en_82_765.txt')\n",
    "bigram_path = pkg_resources.resource_filename('symspellpy', 'frequency_bigramdictionary_en_243_342.txt')\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# Compile regex patterns to ignore in message body contents\n",
    "whitespace_ptrn = re.compile(r'\\s')\n",
    "punctuation_ptrn = re.compile(r'[^\\w\\s\\-]')\n",
    "number_ptrn = re.compile(r'[0-9]')\n",
    "letter_ptrn = re.compile(r'[a-z]')\n",
    "\n",
    "# List of stopwords\n",
    "stopwords = set(['mr', 'dr', 'mrs', 'usd', 'usdm', 'phd', 'hrm', 'ceo', 'drc', 'jk', 'mb', 'kg', 'nnpc', 'es', 'ie', 'ps', 'kabila'])\n",
    "\n",
    "# Iterate over messages and find incorrect spellings\n",
    "for message in messages:\n",
    "    # Add author name to stopwords\n",
    "    if 'Author' in message:\n",
    "        if type(message['Author']) == list:\n",
    "            author = message['Author'][-1]\n",
    "        elif type(message['Author']) == str:\n",
    "            author = message['Author']\n",
    "        author = whitespace_ptrn.sub(' ', author)\n",
    "        author = punctuation_ptrn.sub('', author).replace('_','')\n",
    "        for part in author.split(' '):\n",
    "            if part != '':\n",
    "                stopwords.add(part.lower())\n",
    "    # Count all words\n",
    "    words = 0\n",
    "    # Count misspellings\n",
    "    misspellings = 0\n",
    "    # Normalize and split message content\n",
    "    content = whitespace_ptrn.sub(' ', message['X-TIKA:content'])\n",
    "    content = punctuation_ptrn.sub('', content).replace('_','')\n",
    "    content = number_ptrn.sub('', content)\n",
    "    content = content.lower().split(' ')\n",
    "    # Iterate over split content\n",
    "    for word in content:\n",
    "        # Ignore empty strings and words that do not have any letters (i.e. \"--\")\n",
    "        if word and letter_ptrn.search(word):\n",
    "            # Record word\n",
    "            words += 1\n",
    "            # Don't check spellings of stopwords or words with length <= 1\n",
    "            if word not in stopwords and len(word)>1:\n",
    "                suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "                # Record misspelled words\n",
    "                if len(suggestions) > 1:\n",
    "                    misspellings +=1\n",
    "    # Save ratio of misspellings\n",
    "    message['misspelled_ratio'] = misspellings/words\n",
    "\n",
    "# Save a checkpoint\n",
    "save_to_outfile(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkblue>**Misspellings Distribution**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspells=[]\n",
    "for message in messages:\n",
    "    misspells.append(message[\"misspelled_ratio\"])\n",
    "    \n",
    "mp_df=pd.DataFrame(misspells)\n",
    "mp_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnorm_df=pd.concat([mp_df,pd.DataFrame(msg_len)],axis=1)\n",
    "mnorm_df.columns=[\"misspell_ratio\",\"msg_size\"]\n",
    "mnorm_df[\"norm_ratio\"]=mnorm_df[\"misspell_ratio\"]/mnorm_df[\"msg_size\"]\n",
    "\n",
    "mnorm_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = mnorm_df.boxplot(column=[\"misspell_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = mnorm_df.boxplot(column=[\"norm_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_df.hist(bins=40)   # most misspellings lies between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Ratio of random capitalizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1=read_file(\"opinion_1.txt\")\n",
    "str2=read_file(\"academic_journal_2.txt\")\n",
    "str3=read_file(\"opinion_2.txt\")\n",
    "str4=read_file(\"opinion_4.txt\")\n",
    "str5=read_file(\"opinion_3.txt\")\n",
    "str6=read_file(\"academic_journal_3.txt\")\n",
    "str7=read_file(\"diary_4.txt\")\n",
    "str8=read_file(\"poem_1.txt\")\n",
    "str9=read_file(\"poem_2.txt\")\n",
    "str10=read_file(\"poem_3.txt\")\n",
    "str11=read_file(\"poem_4.txt\")\n",
    "\n",
    "# Create a list of strings from all txt files\n",
    "strlist=[str1,str2,str3,str4,str5,str6,str7,str8,str9,str10,str11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts number of capital letters and periods in a string and returns the ratio\n",
    "def count_period_ratio(string):\n",
    "    \"\"\"Input is a string of article\"\"\"\n",
    "    period_cnt=0\n",
    "    capital_cnt=0\n",
    "    \n",
    "    for letter in string:\n",
    "        if letter==\".\":\n",
    "            period_cnt+=1\n",
    "        elif letter.isupper()==True:\n",
    "            capital_cnt+=1\n",
    "    \n",
    "    ratio=capital_cnt/period_cnt\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of ratios from some test strs (articles, academic journals, poems, and diaries)\n",
    "ratearray=[]\n",
    "testarray=[]\n",
    "\n",
    "for strs in strlist:\n",
    "    ratearray.append(count_period_ratio(strs))\n",
    "    \n",
    "# Take a sample scam email texts to compare thresholds\n",
    "for i in range (1000):\n",
    "    try:\n",
    "        testarray.append(count_period_ratio(messages[i][\"X-TIKA:content\"].strip().lstrip()))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output capitalization ratio summary stats from test strs array\n",
    "df1=pd.DataFrame(ratearray).describe()\n",
    "df1.columns=[\"normal_text\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output capitalization ratio summary stats from sample scam emails array\n",
    "df2=pd.DataFrame(testarray).describe()\n",
    "df2.columns=[\"999_scam_text\"]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkblue>**Visualization of capitalization ratio distribution**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4,))\n",
    "counts, bins, patches = ax.hist(cap_df, facecolor=\"orange\", edgecolor='gray',bins=100)\n",
    "#ax.set_xticks(bins.round(2))\n",
    "#plt.xticks(rotation=70)\n",
    "plt.title('Distribution of capitalization ratio', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "plt.xlabel('Capitalization Ratio', fontsize=10)\n",
    "\n",
    "for patch, leftside, rightside in zip(patches, bins[:-1], bins[1:]):\n",
    "    if rightside < 1:\n",
    "        patch.set_facecolor(\"peachpuff\")\n",
    "    elif leftside > 3.76:\n",
    "        patch.set_facecolor(\"deepskyblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkblue>**Normalized data based on email message size**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_len=[]\n",
    "for message in messages:\n",
    "    msg_len.append(len(message[\"X-TIKA:content\"].strip().lstrip().split(\" \")))\n",
    "\n",
    "norm_df=pd.concat([cap_df,pd.DataFrame(msg_len)],axis=1)\n",
    "norm_df.columns=[\"cap_ratio\",\"msg_size\"]\n",
    "norm_df[\"norm_ratio\"]=norm_df[\"cap_ratio\"]/norm_df[\"msg_size\"]\n",
    "\n",
    "boxplot1 = norm_df.boxplot(column=[\"cap_ratio\"])\n",
    "boxplot2 = norm_df.boxplot(column=[\"norm_ratio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkblue>**Normalized Data Visualization**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>*CHECK NEW BOXPLOT OUTPUTS*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot1 = norm_df.boxplot(column=[\"cap_ratio\"])\n",
    "boxplot2 = norm_df.boxplot(column=[\"norm_ratio\"])\n",
    "\n",
    "# basic plot\n",
    "axs[0, 0].boxplot1(data)\n",
    "axs[0, 0].set_title('Cap_Ratios')\n",
    "\n",
    "# try axs[0, 1].boxplot2(data).set_title('Normalized_Ratios')\n",
    "axs[0, 1].boxplot2(data)\n",
    "axs[0, 1].set_title('Normalized_Ratios')\n",
    "\n",
    "# Multiple box plots on one Axes\n",
    "data = [boxplot1, boxplot1]\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "ax.boxplot(data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ix. Attacker estimated age*\n",
    "- https://github.com/USCDataScience/AgePredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Main Notebook**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *x. Attacker IP known as Phisher?*\n",
    "- https://scamalytics.com/ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>**Madeleine's Notebook**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkblue>**Define functions to lookup IP address and find risk score**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipinfo\n",
    "import random\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_user():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_0_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_0_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:83.0) Gecko/20100101 Firefox/83.0',\t\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 11.0; rv:83.0) Gecko/20100101 Firefox/83.0',\n",
    "        'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) FxiOS/29.0 Mobile/15E148 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (iPad; CPU OS 11_0_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) FxiOS/29.0 Mobile/15E148 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_0_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (iPhone; CPU iPhone OS 14_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def lookup_ip(IP):\n",
    "    url = 'https://scamalytics.com/ip/'\n",
    "    d = {}\n",
    "    try:\n",
    "        page = requests.get(\n",
    "            url + IP,\n",
    "            headers = {'User-Agent': select_user()}\n",
    "        )\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    for text in soup.find_all('pre'):\n",
    "        if 'ip' in text.string:\n",
    "            new_data = json.loads(text.string)\n",
    "            if new_data['ip'] == IP:\n",
    "                d['risk_score'] = new_data['score']\n",
    "                d['risk'] = new_data['risk']\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in messages:\n",
    "    if 'IPInfo-data' in item:\n",
    "        temp = item['IPInfo-data']\n",
    "        append_info = lookup_ip(temp['ip'])\n",
    "        if append_info != {}:\n",
    "            temp['risk_score'] = append_info['risk_score']\n",
    "            temp['risk'] = append_info['risk']\n",
    "            print(temp['ip'], append_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Updated JSON File with Stylometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'fraudulent_emails_features.json'\n",
    "with open(file , 'w') as outfile:\n",
    "    json.dump(messages, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def write_json(data, filename='/Users/madeleine/Desktop/DSCI_550/Assignment_1/Data/fraudulent_emails_update_3_11.json'): \n",
    "    with open(filename,'w') as f: \n",
    "        json.dump(data, f, indent=4)\n",
    "        f.close()\n",
    "    print(\"Data saved to \" + filename.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Analyze and Add Additional Datasets\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1:  Global Unemployment Data\n",
    "#### <font color = darkblue>MIME Top-Level: appplication<br>MIME Sub-level: Excel (.xls)</font>   \n",
    "- Purpose: \n",
    "- Source: *Archived files*\n",
    "\n",
    "#### <font color = darkblue>Unemployment New Features</font>\n",
    "Geographic_NAME:\n",
    "1. Unemployment by Location (geographic_name_unemployment)\n",
    "2. Unemployment by Year when email was sent (geographic_year_unemployment)\n",
    "3. Unemployment by Country (geographic_country_unemployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/madeleine/Desktop/DSCI_550/Assignment_1/Datasets/Global_Unemployment.xls'\n",
    "sheet = 'Data'\n",
    "\n",
    "df = pd.read_excel(filename, sheet_name= sheet)\n",
    "\n",
    "header_row = 2\n",
    "df.columns = df.iloc[header_row]\n",
    "df = df.drop([0,1,header_row])\n",
    "\n",
    "df = df.set_index('Country Name', drop=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_location(location, year, df=df):\n",
    "    d = {}\n",
    "    for i in df['Country Name']:\n",
    "        if i.lower() in location.lower():\n",
    "            d['geographic_name'] = location\n",
    "            d['year'] = year\n",
    "            d['country'] = i\n",
    "            d[str(year) + '_unemployment'] = df.loc[i, year]\n",
    "            d[str(year-5) + '_unemployment'] = df.loc[i, year-5]\n",
    "            d[str(year+5) + '_unemployment'] = df.loc[i, year+5]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in messages:\n",
    "    if 'GeoLocationParser-data' in item:\n",
    "        year = int(item['timestamp'].split('-')[0])\n",
    "        GeoLocations = item['GeoLocationParser-data'][0]#just gets what we need, no need to iterate over 1 item\n",
    "        keys = GeoLocations.keys()\n",
    "        for key in keys:\n",
    "            regex = re.compile('Optional_NAME[\\d]*')\n",
    "            if key == 'Geographic_NAME':\n",
    "                location = GeoLocations['Geographic_NAME']\n",
    "                try:\n",
    "                    to_append = lookup_location(location, year, df)\n",
    "                    if to_append != {}:\n",
    "                        item['GeoLocationParser-data'].append({(key + \"_unemployment\"): to_append})\n",
    "                except:\n",
    "                    print(\"Error: \" + key + \" \" + location)\n",
    "            elif re.match(regex, key):\n",
    "                location = GeoLocations[key]\n",
    "                try:\n",
    "                    to_append = lookup_location(location, year, df)\n",
    "                    if to_append != {}:\n",
    "                        item['GeoLocationParser-data'].append({(key + \"_unemployment\"): to_append})\n",
    "                except:\n",
    "                    print(\"Error: \" + key + \" \" + location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2:  World Bank International Debt Statistics\n",
    "#### <font color = darkblue>MIME Top-Level: text<br>MIME Sub-level: csv</font>   \n",
    "- Purpose: Identify economic debt indicators of (ip_address or geolocation)\n",
    "- Source: *Archived files*\n",
    "\n",
    "#### <font color = darkblue>Debt New Features</font>\n",
    "1. Gross National Income (GNI) in USD\n",
    "2. Total debt service (% of exports of goods, services and primary income)\n",
    "3. Personal transfers and compensation of employees received in USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/madeleine/Desktop/DSCI_550/Assignment_1/Datasets/IDS_CSV/IDS-DRSCountries_WLD_Data.csv'\n",
    "ids_df = pd.read_csv(filename)\n",
    "\n",
    "ids_df = ids_df.set_index('Country Name', drop=False)\n",
    "ids_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = {'GNI_USD':'NY.GNP.MKTP.CD', \n",
    "         'Debt_Service_%':'DT.TDS.DECT.EX.ZS', \n",
    "         'Employee_Compensation_USD':'BX.TRF.PWKR.CD.DT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_loc(location, year, df=ids_df):\n",
    "    countries = set(ids_df['Country Name'])\n",
    "    countries.remove(np.nan)\n",
    "    countries.remove('Last Updated: 10/16/2020')\n",
    "    countries.remove('Least developed countries: UN classification')\n",
    "    countries.remove('Data from database: International Debt Statistics')\n",
    "\n",
    "\n",
    "    d = {}\n",
    "    \n",
    "    if location == 'Federal Republic of Nigeria':\n",
    "        d['country'] = 'Nigeria'\n",
    "        d['geographic_name'] = location\n",
    "        d['year'] = year\n",
    "        df1 = df.loc['Nigeria', ['Series Code', year]]\n",
    "        for key, value in codes.items():\n",
    "            d[year + '_' + key] = df1.loc[df1['Series Code'] == value, year].values[0]\n",
    "    else:\n",
    "        for i in countries:\n",
    "            if i.lower() in location.lower():\n",
    "                d['geographic_name'] = location\n",
    "                d['year'] = year\n",
    "                d['country'] = i\n",
    "                df1 = df.loc[i, ['Series Code', year]]\n",
    "                for key, value in codes.items():\n",
    "                    d[year + '_' + key] = df1.loc[df1['Series Code'] == value, year].values[0]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in messages:\n",
    "    if 'GeoLocationParser-data' in item:\n",
    "        year = item['timestamp'].split('-')[0]\n",
    "        GeoLocations = item['GeoLocationParser-data'][0]\n",
    "        keys = GeoLocations.keys()\n",
    "        for key in keys:\n",
    "            regex = re.compile('Optional_NAME[\\d]*')\n",
    "            if key == 'Geographic_NAME':\n",
    "                location = GeoLocations['Geographic_NAME']\n",
    "                try:\n",
    "                    to_append = lookup_loc(location, year, ids_df)\n",
    "                    if to_append != {}:\n",
    "                        item['GeoLocationParser-data'].append({(key + \"_economic_data\"): to_append})\n",
    "                except:\n",
    "                    print(\"Error: \" + key + \" \" + location)\n",
    "            elif re.match(regex, key):\n",
    "                location = GeoLocations[key]\n",
    "                try:\n",
    "                    to_append = lookup_loc(location, year, ids_df)\n",
    "                    if to_append != {}:\n",
    "                        item['GeoLocationParser-data'].append({(key + \"_economic_data\"): to_append})\n",
    "                except:\n",
    "                    print(\"Error: \" + key + \" \" + location)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3:  Enron Email Corpus\n",
    "#### <font color = darkblue>MIME Top-Level: message<br>MIME Sub-level: message</font>   \n",
    "- Purpose: \n",
    "- Sources:\n",
    "    - Archived files: http://www.enron-mail.com/\n",
    "    - Compressed Data: https://www.cs.cmu.edu/~./enron/\n",
    "\n",
    "#### <font color = darkblue>Enron New Features</font>\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Updated JSON File with Additional Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
